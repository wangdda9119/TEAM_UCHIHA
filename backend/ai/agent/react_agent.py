# ========================================
# LangGraph ReAct Agent - ì™„ì „ ì •ìƒ ì‘ë™ ë²„ì „
# ========================================

from typing import Dict, Any, List, TypedDict

from loguru import logger
from langgraph.graph import StateGraph
from langgraph.constants import END
from langchain_openai import ChatOpenAI
from langchain_core.messages import (
    SystemMessage,
    HumanMessage,
    AIMessage,
    ToolMessage,
)
from langchain_core.tools import BaseTool

from backend.core.config import settings
from backend.ai.tools.search.web_search import web_search
from backend.ai.tools.search.hyupsung_info import uhs_fetch_info
from backend.ai.tools.search.rag_search import rag_search
from backend.ai.agent.prompts.system_prompt import SYSTEM_PROMPT


# ======================================================
# 1) LLM & Tools ì •ì˜
# ======================================================
llm = ChatOpenAI(
    api_key=settings.openai_api_key,
    model="gpt-4o",
    temperature=0.2,
)

# ë„êµ¬ ì •ì˜
TOOLS: List[BaseTool] = [web_search, uhs_fetch_info, rag_search]
TOOL_REGISTRY: Dict[str, BaseTool] = {t.name: t for t in TOOLS}


# ======================================================
# 2) LangGraph State Schema
# ======================================================
class AgentState(TypedDict):
    messages: List[Any]


# ======================================================
# 3) ë…¸ë“œ í•¨ìˆ˜
# ======================================================

def call_agent(state: AgentState) -> Dict[str, Any]:
    """
    LLM í˜¸ì¶œ ë…¸ë“œ
    """
    messages = state["messages"]
    llm_with_tools = llm.bind_tools(TOOLS)

    logger.info("ğŸ§  call_agent ì‹¤í–‰")

    ai_msg = llm_with_tools.invoke(messages)

    # ë©”ì‹œì§€ ì¶”ê°€ í›„ ë°˜í™˜
    return {"messages": messages + [ai_msg]}


def call_tool(state: AgentState) -> Dict[str, Any]:
    """
    Tool í˜¸ì¶œ ë…¸ë“œ
    """
    messages = state["messages"]
    last_msg = messages[-1]

    logger.info("ğŸ”§ call_tool ì‹¤í–‰")

    # tool_callsê°€ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    tool_calls = getattr(last_msg, "tool_calls", None)
    if not tool_calls:
        logger.warning("âš ï¸ tool_calls ì—†ìŒ. agentë¡œ ë³µê·€.")
        return {"messages": messages}

    tc = tool_calls[0]
    tool_name = tc.get("name")
    tool_args = tc.get("args", {})
    tool_call_id = tc.get("id")

    tool = TOOL_REGISTRY.get(tool_name)
    if not tool:
        observation = f"[ERROR] Unknown tool: {tool_name}"
    else:
        try:
            observation = tool.invoke(tool_args)
        except Exception as e:
            observation = f"[ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜] {e}"

    tool_msg = ToolMessage(content=str(observation), tool_call_id=tool_call_id)

    return {"messages": messages + [tool_msg]}


# ======================================================
# 4) Workflow Graph êµ¬ì„±
# ======================================================

workflow = StateGraph(AgentState)

workflow.add_node("agent", call_agent)
workflow.add_node("tool", call_tool)

workflow.set_entry_point("agent")


def should_continue(state: AgentState):
    last = state["messages"][-1]

    tool_calls = getattr(last, "tool_calls", None)

    if tool_calls:
        return "tool"

    return END   # ì¢…ë£Œ


# agent â†’ tool or end
workflow.add_conditional_edges("agent", should_continue)

# tool â†’ agent ë°˜ë³µ
workflow.add_edge("tool", "agent")

app = workflow.compile()


# ======================================================
# 5) ìµœì¢… ì‹¤í–‰ í•¨ìˆ˜
# ======================================================
async def run_react_agent(question: str, memory=None) -> str:
    """
    FastAPIì—ì„œ í˜¸ì¶œí•˜ëŠ” ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ í•¨ìˆ˜.
    memory ì¸ìëŠ” í˜„ì¬ ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ í–¥í›„ í™•ì¥ì„ ìœ„í•´ ë‚¨ê²¨ë‘ .
    """
    logger.info(f"ğŸ¤– run_react_agent: {question}")

    initial_state = {
        "messages": [
            SystemMessage(content=SYSTEM_PROMPT),
            HumanMessage(content=question),
        ]
    }

    result = app.invoke(initial_state)

    final_msg = result["messages"][-1]

    return final_msg.content
