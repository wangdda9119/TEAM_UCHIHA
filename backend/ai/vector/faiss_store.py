from dotenv import load_dotenv
load_dotenv()

import os
import pickle
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter


class FaissStoreBuilder:

    def __init__(self):
        """OpenAI 1024ì°¨ì› í•œêµ­ì–´ ì„ë² ë”© ì´ˆê¸°í™”"""
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # ğŸ”¹ ìµœì‹  OpenAI ì„ë² ë”© (1024ì°¨ì›, í•œêµ­ì–´ ê°•í•¨)
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-large",
            api_key=api_key
        )

        # ğŸ”¹ ë¦¬ì»¤ì‹œë¸Œ ì‹œë©˜í‹± ì²­í‚¹ (600ì / 100ì ì˜¤ë²„ë©)
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=600,
            chunk_overlap=100,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )

        # ğŸ”¹ PDF ëª©ë¡ (ì—¬ê¸° 6ê°œ ë§ìŒ)
        self.pdf_files = [
            "pdfs/ì¥í•™ì œë„.pdf",
            "pdfs/ìˆ˜ê°•ì‹ ì²­ ì•ˆë‚´.pdf",
            "pdfs/ìˆ˜ê°•ì‹ ì²­ ë§¤ë‰´ì–¼.pdf",
            "pdfs/2024_uhs.pdf",
            "pdfs/í˜‘ì„±ëŒ€í•™ì¹™.pdf",
            "pdfs/í†µí•™ë²„ìŠ¤ ì´ìš©ì•ˆë‚´.pdf",
            "pdfs/ê°œì„¤ì‹œê°„í‘œ.pdf"
        ]


 


    # -------------------------------------------------
    # 1) PDF ë¡œë”© + ì²­í¬ ë¶„í•  + ë©”íƒ€ë°ì´í„° ë¶€ì—¬
    # -------------------------------------------------
    def load_documents(self):
        documents = []

        for pdf_path in self.pdf_files:
            if not os.path.exists(pdf_path):
                print(f"âŒ íŒŒì¼ ì—†ìŒ: {pdf_path}")
                continue

            pdf_name = os.path.basename(pdf_path)

            # 1) PDF í˜ì´ì§€ ë‹¨ìœ„ ë¡œë”©
            pages = PyPDFLoader(pdf_path).load()
            print(f"ğŸ“„ {pdf_name} í˜ì´ì§€ ìˆ˜: {len(pages)}")

            # 2) Recursive Text Splitterë¡œ ì²­í¬ ë¶„í• 
            chunks = self.text_splitter.split_documents(pages)

            # 3) ê° ì²­í¬ì— ë©”íƒ€ë°ì´í„° ë¶€ì—¬ + ì „ì²´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            for idx, chunk in enumerate(chunks):
                chunk.metadata = {
                    "type": "pdf",
                    "pdf_name": pdf_name,
                    "source": pdf_path,
                    "chunk_index": idx,
                    "total_chunks": len(chunks),
                }
                documents.append(chunk)

        print(f"ğŸ§© ì´ ì²­í¬ ìˆ˜: {len(documents)}ê°œ ìƒì„±")
        return documents


    


    # -------------------------------------------------
    # 2) ì„ë² ë”© â†’ FAISS ì €ì¥
    # -------------------------------------------------
    def build_faiss_store(self):
        print("\nğŸ”„ PDF â†’ ì„ë² ë”© â†’ FAISS ìƒì„± ì¤‘...\n")

        documents = self.load_documents()
        if len(documents) == 0:
            raise ValueError("âŒ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. PDF ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")

        # ë²¡í„° DB ìƒì„±
        vectorstore = FAISS.from_documents(documents, self.embeddings)

        # ì €ì¥ í´ë” ìƒì„±
        os.makedirs("vectorstore", exist_ok=True)

        # FAISS ì¸ë±ìŠ¤ ì €ì¥
        vectorstore.save_local("vectorstore/index")

        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = [doc.metadata for doc in documents]
        with open("vectorstore/metadata.pkl", "wb") as f:
            pickle.dump(metadata, f)

        print("ğŸ‰ ì„±ê³µ! FAISS VectorStore ì €ì¥ ì™„ë£Œ!\n")


# -------------------------------------------------
# ì‹¤í–‰ ì§„ì…ì 
# -------------------------------------------------
if __name__ == "__main__":
    store = FaissStoreBuilder()
    store.build_faiss_store()
    print("FAISS êµ¬ì¶• ì™„ë£Œ")
